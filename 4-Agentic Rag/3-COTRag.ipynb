{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9589b43",
   "metadata": {},
   "source": [
    "#### Chain Of Thoughts With RAG\n",
    "What is Chain-of-Thought (CoT) in RAG?\n",
    "\n",
    "CoT reasoning breaks down a complex question into intermediate steps, and allows retrieval + reflection at each step before answering.\n",
    "\n",
    "User Query\n",
    "   â†“\n",
    "- Step 1: Decompose question â†’ sub-steps (Reason)\n",
    "- Step 2: Retrieve docs per step (Act)\n",
    "- Step 3: Combine context (Observe)\n",
    "- Step 4: Final answer generation (Reflect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106b7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935c8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Prepare Vectorstore\n",
    "# -------------------------------\n",
    "docs = TextLoader(\"data/research_notes.txt\",encoding=\"utf-8\").load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d26d447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef3a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = init_chat_model(model=\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcde1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. LangGraph State Definition\n",
    "# -------------------------------\n",
    "class RAGCotState(BaseModel):\n",
    "    question: str\n",
    "    sub_steps: List[str] = []\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25e16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Nodes\n",
    "# -------------------------------\n",
    "\n",
    "# a. Plan sub-questions\n",
    "def plan_steps(state:RAGCotState)->RAGCotState:\n",
    "    prompt = f\"Break the question into 2-3 reasoning steps: \\n\\n {state.question}\"\n",
    "    result = llm.invoke(prompt).content\n",
    "    subqs=[line.strip(\"- \") for line in result.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    return state.model_copy(update={\"sub_steps\":subqs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c5993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Retrive for each step\n",
    "def retrieve_per_step(state:RAGCotState)->RAGCotState:\n",
    "    all_docs = []\n",
    "    for sub in state.sub_steps:\n",
    "        docs = retriever.invoke(sub)\n",
    "        all_docs.extend(docs)\n",
    "    return state.model_copy(update={\"retrieved_docs\":all_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69aef1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Generate Final Answer\n",
    "def generate_answer(state: RAGCotState) -> RAGCotState:\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "You are answering a complex question using reasoning and retrieved documents.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Relevant Information:\n",
    "{context}\n",
    "\n",
    "Now synthesize a well-reasoned final answer.\n",
    "\"\"\"\n",
    "    result = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"answer\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fc24214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. LangGraph Graph\n",
    "# -------------------------------\n",
    "builder = StateGraph(RAGCotState)\n",
    "builder.add_node(\"planner\", plan_steps)\n",
    "builder.add_node(\"retriever\", retrieve_per_step)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"retriever\")\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17334a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸªœ Reasoning Steps: ['To break down the question, \"What are the additional experiments in Transformer evaluation?\" into reasoning steps, we can approach it as follows:', '1. **Identify the Purpose of Transformer Evaluation**: Understand what aspects of Transformer models need to be evaluated. This includes examining their performance in tasks like natural language processing (NLP), image processing, or other specific applications.', '2. **Explore Existing Evaluation Metrics and Methods**: Look into the common metrics and benchmarks used to evaluate Transformers, such as accuracy, F1 scores, BLEU scores for translation tasks, and others. Identify gaps or areas where additional experimentation could provide deeper insights.', '3. **Propose Possible Additional Experiments**: Consider new or underexplored aspects that could be evaluated, such as robustness to adversarial examples, efficiency in terms of speed and resource usage, cross-linguistic capabilities, or performance in low-data scenarios.', 'By following these steps, you can form a comprehensive response regarding additional experiments in Transformer evaluation.']\n",
      "\n",
      "âœ… Final Answer:\n",
      " In the context of Transformer model evaluation, several advanced experiments have been conducted to enhance performance, usability, and integration with external tools. Here are the key additional experiments identified:\n",
      "\n",
      "1. **Tool-Augmented Prompting**:\n",
      "   - This approach integrates LangGraph, Wikipedia, and SQL search functionalities. It allows the models to utilize dynamic retrieval-agent reasoning in their responses, enhancing their ability to provide evidence-backed answers. For instance, it enables queries such as \"Give me customer insights from SQL, verify with wiki,\" creating a more interactive and informed chatbot experience.\n",
      "\n",
      "2. **Human Evaluation Protocol**:\n",
      "   - A structured evaluation process has been established where internal annotators score models based on fluency, helpfulness, and correctness. Additionally, GPT-4 is employed as a synthetic evaluator, providing another layer of assessment to ensure robust evaluations of Transformer outputs.\n",
      "\n",
      "3. **FlashAttention2**:\n",
      "   - Integrated into the LLaMA2 model, this method significantly reduces context latency by approximately 50%. This improvement is vital for applications requiring rapid responses, such as real-time interactions in chatbots.\n",
      "\n",
      "4. **Chain-of-Thought Prompting**:\n",
      "   - This experimental prompting technique has shown to outperform direct answer prompting by 8% on logic tasks, indicating that guiding the model through a reasoning process can yield better results. Furthermore, reflective prompting has been found to enhance accuracy by an additional 3%.\n",
      "\n",
      "5. **Retrieval Experiments**:\n",
      "   - These involve testing hybrid dense and sparse retrievers, such as comparing Weaviate with FAISS + BM25 reranking. Results suggest that while FAISS is more efficient, it offers slightly lower recall, which has implications for the choice of retrieval strategies in practice.\n",
      "\n",
      "6. **LoRA Tuning**:\n",
      "   - Low-Rank Adaptation (LoRA) techniques have been used for adapter-based fine-tuning, achieving a significant reduction in GPU memory usage by 60%. This makes it feasible to fine-tune models in environments with limited computational resources while still retaining performance effectiveness.\n",
      "\n",
      "7. **Safety Protocols**:\n",
      "   - Enhancements to safety measures include implementing toxicity detection through tools like Detoxify and utilizing a zero-shot classifier for out-of-scope filtering. Moreover, red teaming practices, which involve adversarial prompt testing, have been integrated to further refine the models' resilience against harmful interactions.\n",
      "\n",
      "In summary, these additional experiments reflect a multifaceted effort to not only improve the intrinsic qualities of Transformer models, such as their reasoning capabilities and operational efficiency but also to ensure their applicability in real-world scenarios while maintaining high safety standards.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5. Run CoT RAG Agent\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"what are the additional eperiments in Transformer eveluation?\"\n",
    "    state = RAGCotState(question=query)\n",
    "    final = graph.invoke(state)\n",
    "\n",
    "    print(\"\\nðŸªœ Reasoning Steps:\", final[\"sub_steps\"])\n",
    "    print(\"\\nâœ… Final Answer:\\n\", final[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
