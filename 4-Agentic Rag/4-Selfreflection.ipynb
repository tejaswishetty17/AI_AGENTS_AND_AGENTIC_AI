{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4408d7c3",
   "metadata": {},
   "source": [
    "### üß† What is Self-Reflection in RAG?\n",
    "Self-reflection = LLM evaluates its own output:\n",
    "‚ÄúIs this clear, complete, and accurate?‚Äù\n",
    "\n",
    "#### Self-Reflection in RAG using LangGraph, we‚Äôll design a workflow where the agent:\n",
    "\n",
    "1. Generates an initial answer using retrieved context\n",
    "2. Reflects on that answer with a dedicated self-critic LLM step\n",
    "3. If unsatisfied, it can revise the query, retrieve again, or regenerate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b72c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langgraph.graph import StateGraph, END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11c66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load llm models\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a86572",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(\"data/internal_docs.txt\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap=50).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e55e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. State Definition\n",
    "# -------------------------\n",
    "class RAGReflectionState(BaseModel):\n",
    "    question:str\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\"\n",
    "    reflection:str = \"\"\n",
    "    revised: bool = False\n",
    "    attempts: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd48df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Nodes\n",
    "# -------------------------\n",
    "\n",
    "# a. Retrieve\n",
    "def retrieve_docs(state:RAGReflectionState)-> RAGReflectionState:\n",
    "    docs = retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"retrieved_docs\":docs})\n",
    "\n",
    "#b. Generate Answer\n",
    "def generate_answer(state: RAGReflectionState)->RAGReflectionState:\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"Use the following context to answer the question:\n",
    "    context:\n",
    "    {context} \n",
    "    \n",
    "    Question:\n",
    "    {state.question}\n",
    "    \"\"\"\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update = {\"answer\": answer, \"attempts\":state.attempts + 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f845696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Self-Reflection\n",
    "def reflect_on_answer(state:RAGReflectionState)->RAGReflectionState:\n",
    "    prompt = f\"\"\"\n",
    "Reflect on the following answer to see if it fully addresses the question.\n",
    "State Yes if it is complete and correct, or NO with an explanation.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Answer: {state.answer}\n",
    "\n",
    "Respond like:\n",
    "Reflection: YES or NO\n",
    "Explanation:...\n",
    "\"\"\" \n",
    "    result = llm.invoke(prompt).content\n",
    "    is_ok = \"reflection: yes\" in result.lower()\n",
    "    return state.model_copy(update={\"reflection\":result, \"revised\": not is_ok})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4af380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d. Finalizer\n",
    "def finalize(state: RAGReflectionState)->RAGReflectionState:\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2e57e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. LangGraph DAG\n",
    "# -------------------------\n",
    "\n",
    "builder = StateGraph(RAGReflectionState)\n",
    "\n",
    "builder.add_node(\"retriever\", retrieve_docs)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "builder.add_node(\"reflector\", reflect_on_answer)\n",
    "builder.add_node(\"done\", finalize)\n",
    "\n",
    "builder.set_entry_point(\"retriever\")\n",
    "\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", \"reflector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    lambda s: \"done\" if not s.revised or s.attempts>=2 else \"retriever\"\n",
    ")\n",
    "\n",
    "builder.add_edge(\"done\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5598b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Final Answer:\n",
      " The Transformer variants mentioned for production deployments are:\n",
      "\n",
      "1. **EfficientFormer** - Focuses on reducing computational overhead while maintaining performance.\n",
      "2. **Longformer** - Suitable for handling long document contexts.\n",
      "3. **Reformer** - Designed for efficiency with reduced compute and memory requirements.\n",
      "4. **LLaMA2** - An adaptable, open-source model that is fine-tunable for various applications.\n",
      "\n",
      "These variants are chosen based on factors like input sequence length, resource availability, and application requirements.\n",
      "\n",
      "üîÅ Reflection Log:\n",
      " Reflection: YES  \n",
      "Explanation: The answer satisfactorily identifies four transformer variants used in production deployments and briefly describes their main characteristics and advantages. It also mentions the criteria for choosing the appropriate variant based on specific project needs, providing a comprehensive understanding of the topic.\n",
      "üîÑ Total Attempts: 2\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5. Run the Agent\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What are the transformer variants in production deployments?\"\n",
    "    init_state = RAGReflectionState(question=user_query)\n",
    "    result = graph.invoke(init_state)\n",
    "\n",
    "    print(\"\\nüß† Final Answer:\\n\", result[\"answer\"])\n",
    "    print(\"\\nüîÅ Reflection Log:\\n\", result[\"reflection\"])\n",
    "    print(\"üîÑ Total Attempts:\", result[\"attempts\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
