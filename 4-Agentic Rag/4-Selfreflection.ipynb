{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4408d7c3",
   "metadata": {},
   "source": [
    "### üß† What is Self-Reflection in RAG?\n",
    "Self-reflection = LLM evaluates its own output:\n",
    "‚ÄúIs this clear, complete, and accurate?‚Äù\n",
    "\n",
    "#### Self-Reflection in RAG using LangGraph, we‚Äôll design a workflow where the agent:\n",
    "\n",
    "1. Generates an initial answer using retrieved context\n",
    "2. Reflects on that answer with a dedicated self-critic LLM step\n",
    "3. If unsatisfied, it can revise the query, retrieve again, or regenerate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b72c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11c66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load llm models\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a86572",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(\"data/internal_docs.txt\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap=50).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e55e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. State Definition\n",
    "# -------------------------\n",
    "class RAGReflectionState(BaseModel):\n",
    "    question:str\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\"\n",
    "    reflection:str = \"\"\n",
    "    revised: bool = False\n",
    "    attempts: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd48df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Nodes\n",
    "# -------------------------\n",
    "\n",
    "# a. Retrieve\n",
    "def retrieve_docs(state:RAGReflectionState)-> RAGReflectionState:\n",
    "    docs = retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"retrieved_docs\":docs})\n",
    "\n",
    "#b. Generate Answer\n",
    "def generate_answer(state: RAGReflectionState)->RAGReflectionState:\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"Use the following context to answer the question:\n",
    "    context:\n",
    "    {context} \n",
    "    \n",
    "    Question:\n",
    "    {state.question}\n",
    "    \"\"\"\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update = {\"answer\": answer, \"attempts\":state.attempts + 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f845696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Self-Reflection\n",
    "def reflect_on_answer(state:RAGReflectionState)->RAGReflectionState:\n",
    "    prompt = f\"\"\"\n",
    "Reflect on the following answer to see if it fully addresses the question.\n",
    "State Yes if it is complete and correct, or NO with an explanation.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Answer: {state.answer}\n",
    "\n",
    "Respond like:\n",
    "Reflection: YES or NO\n",
    "Explanation:...\n",
    "\"\"\" \n",
    "    result = llm.invoke(prompt).content\n",
    "    is_ok = \"reflection: yes\" in result.lower()\n",
    "    return state.model_copy(update={\"reflection\":result, \"revised\": not is_ok})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4af380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d. Finalizer\n",
    "def finalize(state: RAGReflectionState)->RAGReflectionState:\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2e57e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. LangGraph DAG\n",
    "# -------------------------\n",
    "\n",
    "builder = StateGraph(RAGReflectionState)\n",
    "\n",
    "builder.add_node(\"retriever\", retrieve_docs)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "builder.add_node(\"reflector\", reflect_on_answer)\n",
    "builder.add_node(\"done\", finalize)\n",
    "\n",
    "builder.set_entry_point(\"retriever\")\n",
    "\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", \"reflector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    lambda s: \"done\" if not s.revised or s.attempts>=2 else \"retriever\"\n",
    ")\n",
    "\n",
    "builder.add_edge(\"done\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5598b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Final Answer:\n",
      " The Transformer variants mentioned in the context for production deployments are:\n",
      "\n",
      "1. **EfficientFormer** - Focused on reducing computational overhead while maintaining performance.\n",
      "2. **Longformer** - Designed to handle long input sequences effectively.\n",
      "3. **Reformer** - Optimized for efficiency in compute and memory utilization.\n",
      "4. **LLaMA2** - An adaptable model that is open-source and fine-tunable for various applications.\n",
      "\n",
      "These models each have unique trade-offs that make them suitable for different production scenarios based on factors like scalability, efficiency, adaptability, and deployment environment.\n",
      "\n",
      "üîÅ Reflection Log:\n",
      " Reflection: NO  \n",
      "Explanation: The answer lists several transformer variants but does not cover the full breadth of transformer variants commonly used in production, such as T5, BERT, GPT (including GPT-3 and GPT-4), and others. Additionally, while it mentions unique features and trade-offs of some variants, it lacks a more comprehensive overview of how these models compare and their specific use cases in production deployments. A more complete answer would include a broader range of transformer models and elaborate on their implications in real-world applications.\n",
      "üîÑ Total Attempts: 2\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5. Run the Agent\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What are the transformer variants in production deployments?\"\n",
    "    init_state = RAGReflectionState(question=user_query)\n",
    "    result = graph.invoke(init_state)\n",
    "\n",
    "    print(\"\\nüß† Final Answer:\\n\", result[\"answer\"])\n",
    "    print(\"\\nüîÅ Reflection Log:\\n\", result[\"reflection\"])\n",
    "    print(\"üîÑ Total Attempts:\", result[\"attempts\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
